<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ml on Home</title><link>https://antopolskiy.github.io/tags/ml/</link><description>Recent content in ml on Home</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 01 Oct 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://antopolskiy.github.io/tags/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Kickstart text classification with GPT-3, part 1</title><link>https://antopolskiy.github.io/post/gpt3_1/</link><pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate><guid>https://antopolskiy.github.io/post/gpt3_1/</guid><description>Motivation In the life of any data scientist there comes a point when you realize that you don&amp;rsquo;t have labelled data for the problem you&amp;rsquo;re trying to solve. In this series of posts, I propose one approach to generate labelled data for text classification using large pretrained neural networks.
TL;DR Gives an brief introduction to GPT-3; discusses a way to think about text completion; explains what is zero-shot, few-show classification and fine-tuning; introduces the example dataset that will be used in this series of posts; gives several examples of a naive approach to completion.</description></item></channel></rss>