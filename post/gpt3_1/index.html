<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Kickstart text classification with GPT-3, part 1</title><style>html body{font-family:source sans pro,sans-serif;background-color:#fff;font-weight:200}:root{--accent:#9c27b0;--border-width:5px}</style><link rel=stylesheet href=https://antopolskiy.github.io/css/main.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source%20Sans%20Pro"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js></script>
<script>hljs.initHighlightingOnLoad()</script><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js></script>
<script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script>
<script>$(document).on("click",function(){$(".collapse").collapse("hide")})</script><meta name=generator content="Hugo 0.104.2"><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></head><body><nav class="navbar navbar-default navbar-fixed-top"><div class=container><div class=navbar-header><a class="navbar-brand visible-xs" href=#>Kickstart text classification with GPT-3, part 1</a>
<button class=navbar-toggle data-target=.navbar-collapse data-toggle=collapse>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button></div><div class="collapse navbar-collapse"><ul class="nav navbar-nav"><li><a href=/>Home</a></li><li><a href=/post/>Posts</a></li></ul><ul class="nav navbar-nav navbar-right"><li class=navbar-icon><a href=mailto:s.antopolsky@gmail.com><i class="fas fa-envelope"></i></a></li><li class=navbar-icon><a href=https://github.com/antopolskiy/><i class="fab fa-github"></i></a></li><li class=navbar-icon><a href=https://twitter.com/antopolskiy/><i class="fab fa-twitter"></i></a></li><li class=navbar-icon><a href=https://www.linkedin.com/in/antop/><i class="fab fa-linkedin"></i></a></li><li class=navbar-icon><a href=https://stackoverflow.com/users/3033945/sergey-antopolskiy><i class="fab fa-stack-overflow"></i></a></li></ul></div></div></nav><main><div class=content><h2>Kickstart text classification with GPT-3, part 1</h2><h5>Pubished: October 1, 2022</h5><a href=https://antopolskiy.github.iotags/ml><kbd class=item-tag>ml</kbd></a>
<a href=https://antopolskiy.github.iotags/nlp><kbd class=item-tag>nlp</kbd></a>
<a href=https://antopolskiy.github.iotags/gpt3><kbd class=item-tag>gpt3</kbd></a></div><div align=start class=content><h2 id=motivation>Motivation</h2><p>In the life of any data scientist there comes a point when you realize that you don&rsquo;t have labelled data for the problem you&rsquo;re trying to solve. In this series of posts, I propose one approach to generate labelled data for text classification using large pretrained neural networks.</p><h2 id=tldr>TL;DR</h2><p>Gives an brief introduction to GPT-3; discusses a way to think about text completion; explains what is zero-shot, few-show classification and fine-tuning; introduces the example dataset that will be used in this series of posts; gives several examples of a naive approach to completion.</p><h2 id=what-is-gpt-3>What is GPT-3?</h2><p>GPT stands for “Generative Pre-trained Transformer”, and &ldquo;3&rdquo; is simply the version. There are <a href=https://medium.com/sciforce/what-is-gpt-3-how-does-it-work-and-what-does-it-actually-do-9f721d69e5c1>many</a> <a href=https://en.wikipedia.org/wiki/GPT-3>good</a> <a href=https://towardsdatascience.com/understanding-gpt-3-in-5-minutes-7fe35c3a1e52>resources</a> describing what it is, and what it does. I won&rsquo;t go into these details here.</p><p>In this series of posts, we can treat GPT-3 as a black box. There are a few important things to keep in mind:</p><ol><li>GPT-3 does text completion. Whatever you give it, it will try to add text to it until it reaches either the token number limit you set or generates one of the stop-words/symbols you’ve specified (e.g. new line symbol).</li><li>GPT-3 was trained on a huge amount of text mostly scraped from the web<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</li></ol><h3 id=accessing-gpt-3>Accessing GPT-3</h3><p>You can access GPT-3 on <a href=https://beta.openai.com/>OpenAI&rsquo;s website</a>. It has a nice interface and an API, both of which we will be using later.</p><p>GPT-3 is not free, but OpenAI gives you credits on sign-up to explore the potential applications, and it is enough for a few small-scale projects. In general, these kinds of engines will become cheaper as time goes on.</p><h4 id=a-note-on-different-engines>A note on different engines</h4><p>There are <a href=https://beta.openai.com/docs/models/gpt-3>several versions of the GPT-3 models</a> (sometimes referred to as &ldquo;engines&rdquo;). The differences are based on the size of the network and training data. Smaller networks work faster and consume fewer resources and are therefore cheaper. But the completions are dumber. We will be using the most capable engine Davinci, because we need to squeeze all the smarts we can from the network.</p><h2 id=how-i-think-about-text-completion>How I think about text completion</h2><p>So what can we do with GPT-3? Well, we give GPT-3 some initial text, called <em><strong>prompt</strong></em>, and GPT-3 completes it. What kinds of problems does it help us to solve?</p><p>Turns out, a lot of problems can be reframed as text completion problems. This is where creativity and imagination will be of use. OpenAI gives many <a href=https://beta.openai.com/examples>examples</a>, such as Keyword extraction, Text summarization, and more. The process of reframing the problem you want to solve into a text completion problem is called <em><strong>prompt engineering</strong></em><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><p>But how to come up with good prompts? By now, prompt engineering has hundreds of posts written about it, and soon there will be dedicated books. However, as we just want to get started, I offer you a way to think about prompts that has helped me on many occasions. I call it <strong>&ldquo;it&rsquo;s gotta be somewhere on the internet&rdquo;</strong>.</p><p>Whenever you create a prompt, think about whether the result (your prompt + completion) could be the content of a page somewhere on the internet. This idea comes from the fact that the training dataset for GPT-3 included a lot of web page content.</p><p>The Internet is a weird place, and there is life beyond the first couple of pages of Google Search results. The Internet is full of <a href=https://homepages.inf.ed.ac.uk/rbf/HIPR2/gsmooth.htm>old university pages</a>, <a href=https://stat.ethz.ch/pipermail/r-sig-finance/2012q1/009645.html>dumps of mailing lists</a>, <a href=https://ijpds.org/article/view/1680>articles containing tables with data</a>, and other interesting things. In the next post, I will show how I use table format to create an efficient <em><strong>zero-shot</strong></em> classifier. Wait, what&rsquo;s zero-shot?</p><h2 id=zero-shot-few-shot-and-fine-tuning>Zero-shot, few-shot and fine-tuning</h2><p>There are a few terms you need to know if you&rsquo;re going to use GPT-3 for classification.</p><p><em><strong>Zero-shot</strong></em> simply means that you use GPT-3 for completion without giving specific examples of what you want to do. In essence, you rely on what the network has already learned during its training. An example of using zero-shot classification to infer the genre of a Steam game from description<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> (<span style=background-color:#b9eebc>
GPT completion is highlighted
</span>):</p><div style="border:.5px solid #d0d1d8;border-radius:3px;color:#4a4c52;padding:15px;font-family:helveticaneue-light,sans-serif;margin-top:2em;margin-bottom:2em;font-size:18px;height:100%"><p>Game description:</p><p>Carefully guide your nation from the era of absolute monarchies in the early 19th century, through expansion and colonization, to finally become a truly great power by the dawn of the 20th century. Victoria II is a grand strategy game played during the colonial era of the 19th century, where the player takes control of a country, guiding...</p><p>Is this a strategy game? Yes or No: <span style=background-color:#b9eebc>Yes</span></p></div><p><em><strong>Few-shot</strong></em> is when <em>in the prompt</em> you provide one or more examples of what you want GPT-3 to do. This can be quite powerful, especially when the completion you want is non-trivial, and after a few tries, you see that GPT-3 doesn&rsquo;t make it correctly after a zero-shot try. This can also improve the stability of the response: by providing an example, you all but ensure that the rest of the response will be in the same format. But there are better ways of ensuring stability which I will discuss in future posts. Few-shot comes with additional challenges, such as the dependency of the responses on the example(s) you&rsquo;ve provided. Example of 1-shot classification:</p><div style="border:.5px solid #d0d1d8;border-radius:3px;color:#4a4c52;padding:15px;font-family:helveticaneue-light,sans-serif;margin-top:2em;margin-bottom:2em;font-size:18px;height:100%"><p>Game description:<p>Carefully guide your nation from the era of absolute monarchies in the early 19th century, through expansion and colonization, to finally become a truly great power by the dawn of the 20th century. Victoria II is a grand strategy game played during the colonial era of the 19th century, where the player takes control of a country, guiding...<p>Is this a strategy game? Yes or No: Yes<p>Game description:<p>Tactical Squad-Based Combat comes to the Fallout® Universe! You are the wretched refuse. You may be born from dirt, but we will forge you into steel. You will learn to bend; if not you, will you break. In these dark times, the Brotherhood - your Brotherhood - is all that stands between the rekindled flame of civilization and the howling,...<p>Is this a strategy game? Yes or No: <span style=background-color:#b9eebc>Yes</span></div><p><em><strong>Fine-tuning</strong></em> refers to modifying the underlying network using a labelled dataset <em>before prompting for completion</em>. It allows turning a general-purpose network, such as GPT-3, into a very specific model. It requires having some labelled data upfront, and it is more expensive if you want to do it with GPT-3. You can read more about GPT-3 fine-tuning <a href=https://beta.openai.com/docs/guides/fine-tuning>here</a>.</p><h2 id=a-naive-approach-to-zero-shot-classification>A naive approach to zero-shot classification</h2><p>Consider the examples from the <a href=#zero-shot-few-shot-and-fine-tuning>Zero-shot section above</a>. We can see how GPT-3 can helps us do classification. However, it seems cumbersome to make requests for each of the samples, and each of the categories. Are there better ways?</p><h3 id=multiple-examples>Multiple examples</h3><p>We can pack multiple examples in the same request, like so:</p><div style="border:.5px solid #d0d1d8;border-radius:3px;color:#4a4c52;padding:15px;font-family:helveticaneue-light,sans-serif;margin-top:2em;margin-bottom:2em;font-size:18px;height:100%"><p># Game Descriptions<p>1. Carefully guide your nation from the era of absolute monarchies in the early 19th century, through expansion and colonization, to finally become a truly great power by the dawn of the 20th century. Victoria II is a grand strategy game played during the colonial era of the 19th century, where the player takes control of a country, guiding…<p>2. Build the adventure from Privet Drive to the Triwizard Tournament and experience the magic of the first four Harry Potter stories – LEGO style! Explore Hogwarts™ School of Witchcraft and Wizardry, learn spells, brew potions and relive the adventures like never before with tongue-in-cheek humor and creative customization that is unique to...<p>3. Sudeki, a world torn apart into light, shadow and dark. Rent asunder by deceit and betrayal, the land cries out for a peace that only four united heroes can bring. Beyond the protective walls of Illumina Castle lie miles of pastoral, rolling countryside, though once a serene landscape, the Aklorian forces have transformed the countryside...<p>4. Mini Ninjas is a game that combines furious action with stealth and exploration for an experience that appeals to a wide audience across age groups and preferences. It’s an action-adventure with a strong focus on allowing the player freedom to explore the world and has the depth to allow for very varied gameplay and approaches to getting...<p>5. Life under the rule of the winged Skyborn race isn't so bad for Claret Spencer, the star mechanic of an independent repair shop. She can patch up just about anything...but when a certain cravat-wearing customer turns her life upside-down, she finds herself pulled into an epic, city-wide conflict that's going to take a lot more than elbow...<p># Is this a strategy game? (Yes or No)<p>1. <span style=background-color:#b9eebc>Yes<br>2. No<br>3. No<br>4. No<br>5. No</span></div><p>As you can see, we can get predictions for multiple games in the same request.</p><p>However, what if we want to get predictions for multiple games and multiple labels (genres) at the same time? In the next post, I will show how to use the markdown tables structure to achieve this goal.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>See <a href=https://commoncrawl.org/>Common Crawl</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Incidentally, prompt engineering is relevant not only for text completion such as GPT-3 but also for image generation networks, such as DALLE and Midjourney. It has gotten to the point and many people consider prompt engineering as its own way of programming, and predict that shortly we will have a whole profession of &ldquo;prompt engineers&rdquo;.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>In this series of posts I will be using the dataset of <a href="https://www.kaggle.com/datasets/trolukovich/steam-games-complete-dataset?resource=download">Steam games descriptions and tags</a>. A subset of the most popular tags will be used as genres. Of course, the whole point of this approach is to label <em>unlabelled</em> data, however, for demonstration and validation of the approach, it is important to have labelled data.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><h4 class=page-header>Comments</h4><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//https-antopolskiy-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></main><footer><p class="copyright text-muted">© Sergey Antopolskiy, 2022</p></footer></body></html>